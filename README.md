
# ETL_PYSPARK_GLUE

Projeto de pipeline de dados utilizando **PySpark**, **AWS Glue**, **Amazon S3** e **Terraform** para ingestÃ£o e transformaÃ§Ã£o de dados em um data lake na AWS.

## ğŸš€ Tecnologias

- **Python 3.x**
- **PySpark**
- **AWS S3**
- **AWS Glue**
- **Terraform**

## ğŸ“‚ Estrutura do Projeto

```
ETL_PYSPARK_GLUE/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clientes.csv
â”‚   â””â”€â”€ vendas.txt
â”œâ”€â”€ infra_terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ingesta_bronze/
â”‚   â”œâ”€â”€ insert_bronze_clientes.py
â”‚   â””â”€â”€ insert_bronze_vendas.py
â”œâ”€â”€ silver_transformation/
â”‚   â””â”€â”€ resumo_clientes_balanco_produtos.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ max_partition.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”‘ PrÃ©-requisitos

- AWS CLI configurado com acesso Ã  conta AWS
- Terraform instalado
- Python 3.x com PySpark instalado localmente (para testes, se necessÃ¡rio)
- PermissÃµes no AWS S3, Glue e Athena

## âš™ï¸ ConfiguraÃ§Ã£o inicial

### Configurar credenciais AWS

```bash
aws configure
```

Informe:
- Access Key ID
- Secret Access Key
- RegiÃ£o (ex: `us-east-1`)
- Formato de saÃ­da (opcional: `json`)

### Provisionar infraestrutura com Terraform

No diretÃ³rio `infra_terraform/`:

```bash
terraform init
terraform plan
terraform apply
```

## ğŸ“¤ Upload dos arquivos para o S3

### Dados

```bash
aws s3 cp data/clientes.csv s3://bucket-clientes-vendas-py/data/clientes.csv
aws s3 cp data/vendas.txt s3://bucket-clientes-vendas-py/data/vendas.txt
```

### Scripts

```bash
aws s3 cp ingesta_bronze/insert_bronze_clientes.py s3://bucket-clientes-vendas-py/scripts/ingesta_bronze/bronze_clientes.py
aws s3 cp ingesta_bronze/insert_bronze_vendas.py s3://bucket-clientes-vendas-py/scripts/ingesta_bronze/bronze_vendas.py
aws s3 cp silver_transformation/resumo_clientes_balanco_produtos.py s3://bucket-clientes-vendas-py/scripts/transformation/resumo_clientes_balanco_produtos.py
```

## ğŸ› ï¸ ExecuÃ§Ã£o dos Glue Jobs

1. Acesse o AWS Glue â†’ ETL Jobs
2. Execute os jobs:
   - `ingesta_bronze_clientes`: cria a tabela `bronze.clientes_bronze`
   - `ingesta_bronze_vendas`: cria a tabela `bronze.vendas_bronze`
   - `pipeline_clientes_produtos`: gera `silver.resumo_clientes` e `silver.balanco_produtos`

Logs disponÃ­veis no **CloudWatch Logs**.

## ğŸ—ï¸ Atualizar metadados no Glue Catalog

```sql
MSCK REPAIR TABLE bronze.clientes_bronze;
MSCK REPAIR TABLE bronze.vendas_bronze;
MSCK REPAIR TABLE silver.resumo_clientes;
MSCK REPAIR TABLE silver.balanco_produtos;
```

## ğŸ“¦ DependÃªncias

Exemplo do `requirements.txt`:

```
boto3
pyspark
```

## ğŸ“Œ ObservaÃ§Ãµes

- Substitua `bucket-clientes-vendas-py` pelo seu bucket real.
- Adapte os nomes no Glue/Athena conforme o Data Catalog.
