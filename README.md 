
# ETL_PYSPARK_GLUE

Projeto de pipeline de dados utilizando **PySpark**, **AWS Glue**, **Amazon S3** e **Terraform** para ingestÃ£o e transformaÃ§Ã£o de dados em um data lake na AWS.

## ğŸš€ Tecnologias

- **Python 3.x**
- **PySpark**
- **AWS S3**
- **AWS Glue**
- **Terraform**

## ğŸ“‚ Estrutura do Projeto

```
ETL_PYSPARK_GLUE/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clientes.csv
â”‚   â””â”€â”€ vendas.txt
â”œâ”€â”€ infra_terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ingesta_bronze/
â”‚   â”œâ”€â”€ insert_bronze_clientes.py
â”‚   â””â”€â”€ insert_bronze_vendas.py
â”œâ”€â”€ silver_transformation/
â”‚   â””â”€â”€ resumo_clientes_balanco_produtos.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ max_partition.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”‘ PrÃ©-requisitos

- AWS CLI configurado com acesso Ã  conta AWS
- Terraform instalado
- Python 3.x com PySpark instalado localmente (para testes, se necessÃ¡rio)
- PermissÃµes no AWS S3, Glue e Athena

## âš™ï¸ ConfiguraÃ§Ã£o inicial

### 1ï¸âƒ£ Configurar credenciais AWS

```bash
aws configure
```
Informe:
- **Access Key ID**
- **Secret Access Key**
- **RegiÃ£o** (ex: `us-east-1`)
- **Formato de saÃ­da** (opcional: `json`)

### 2ï¸âƒ£ Provisionar infraestrutura com Terraform

Dentro do diretÃ³rio `infra_terraform/`:
```bash
terraform init
terraform plan
terraform apply
```
Isso irÃ¡ provisionar os recursos necessÃ¡rios (ex.: buckets, permissÃµes).

## ğŸ“¤ Upload dos arquivos para o S3

### Dados
```bash
aws s3 cp data/clientes.csv s3://bucket-clientes-vendas-py/data/clientes.csv
aws s3 cp data/vendas.txt s3://bucket-clientes-vendas-py/data/vendas.txt
```

### Scripts de ingestÃ£o e transformaÃ§Ã£o
```bash
aws s3 cp ingesta_bronze/insert_bronze_clientes.py s3://bucket-clientes-vendas-py/scripts/ingesta_bronze/bronze_clientes.py
aws s3 cp ingesta_bronze/insert_bronze_vendas.py s3://bucket-clientes-vendas-py/scripts/ingesta_bronze/bronze_vendas.py
aws s3 cp silver_transformation/resumo_clientes_balanco_produtos.py s3://bucket-clientes-vendas-py/scripts/transformation/resumo_clientes_balanco_produtos.py
```

## ğŸ› ï¸ ExecuÃ§Ã£o dos Glue Jobs

1ï¸âƒ£ **Acesse o AWS Glue â†’ ETL Jobs**

2ï¸âƒ£ **Execute os jobs na seguinte ordem:**

- `ingesta_bronze_clientes`  
  > Ingesta do CSV de clientes â†’ cria a tabela `bronze.clientes_bronze`

- `ingesta_bronze_vendas`  
  > Ingesta do TXT de vendas â†’ cria a tabela `bronze.vendas_bronze`

- `pipeline_clientes_produtos`  
  > Realiza transformaÃ§Ã£o e gera:  
  - `silver.resumo_clientes`
  - `silver.balanco_produtos`

ğŸ’¡ **Logs:** Os logs de execuÃ§Ã£o dos jobs podem ser consultados no **CloudWatch Logs**.

## ğŸ—ï¸ Atualizar metadados no Glue Catalog (Athena)

ApÃ³s execuÃ§Ã£o dos jobs, rode no Athena:
```sql
MSCK REPAIR TABLE bronze.clientes_bronze;
MSCK REPAIR TABLE bronze.vendas_bronze;
MSCK REPAIR TABLE silver.resumo_clientes;
MSCK REPAIR TABLE silver.balanco_produtos;
```
âœ… Isso garante que partiÃ§Ãµes criadas sejam reconhecidas.

## ğŸ“¦ DependÃªncias

Exemplo de `requirements.txt` (para uso local ou Glue Python Shell):
```
boto3
pyspark
```

## âœ… ExecuÃ§Ã£o do pipeline

- Os scripts sÃ£o executados via AWS Glue apontando para os arquivos no S3.


## ğŸ“Œ ObservaÃ§Ãµes

- Substitua `bucket-clientes-vendas-py` pelo nome real do seu bucket.
- Adapte os nomes das tabelas no Glue/Athena conforme o Data Catalog criado.
- Certifique-se de que as permissÃµes no S3 e Glue estÃ£o corretamente configuradas.